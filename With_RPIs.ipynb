{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWhlzioX8blq8yW8AJIflG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArianaMarta/small_pysyft/blob/main/With_RPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY02xS-ShJaL"
      },
      "outputs": [],
      "source": [
        "!pip install syft==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "fF1v5MXshKS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the syft library\n",
        "import syft as sy"
      ],
      "metadata": {
        "id": "M40GU2jjhVDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## THE OTHER NOTEBOOK ##\n",
        "\n",
        "# hook torch with syft to add extra funcionalities \n",
        "# to support Federated Learning and other private AI tools\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# create two virtual workers, in this case two schools for example\n",
        "# they will hold the data while training the model locally\n",
        "aalto_school = sy.VirtualWorker(hook, id=\"aalto\")\n",
        "tampere_school = sy.VirtualWorker(hook, id=\"tampere\")"
      ],
      "metadata": {
        "id": "TRlIM4Xrn-EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hook PyTorch\n",
        "hook = sy.TorchHook(torch)  \n",
        "# When using raspberry pi \n",
        "hook = sy.TorchHook(torch) \n",
        "\n",
        "kwargs_websocket_aalto = {\"host\": \"ip_aalto\", \"hook\": hook}\n",
        "aalto_school = WebsocketClientWorker(id=\"aalto\", port=8777, **kwargs_websocket_aalto)\n",
        "\n",
        "kwargs_websocket_tampere = {\"host\": \"ip_tampere\", \"hook\": hook}\n",
        "tampere_school = WebsocketClientWorker(id=\"tampere\", port=8778, ** kwargs_websocket_tampere)\n",
        "\n",
        "virtual_workers = [aalto_school,tampere_school]"
      ],
      "metadata": {
        "id": "Yf3kwnTMn9DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets load the data \n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, ), (0.5, )), # Normalize a tensor image with mean and standard deviation\n",
        "])\n",
        "\n",
        "train_set = datasets.MNIST(\n",
        "    \"../data\", train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(\n",
        "    \"../data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# transform the data into a federated dataset using .federate()\n",
        "# the federate() method splits the data within the workers\n",
        "federated_train_loader = sy.FederatedDataLoader(                  \n",
        "    train_set.federate((aalto_school, tampere_school)), batch_size=64, shuffle=True) \n",
        "\n",
        "# test data remains with us locally\n",
        "# so we use torch.utils.data.DataLoader as we normally did \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "wonyXbGBiaPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets create a simple Net\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 784-dim tensor of pixel values for each image (28*28 sized images)\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        # producing a tensor of length 10 which indicates the class scores for an input image (0-9)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "krho-XwcBrHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ori = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epoch = 10\n",
        "model_virtual = {}\n",
        "\n",
        "for worker in virtual_workers:\n",
        "  model_copied = model_ori.copy()\n",
        "  model_ptr = model_copied.send(worker)\n",
        "  model_virtual[worker.id] = model_ptr\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  #model.train()\n",
        "  \n",
        "  for batch_index, (data, target) in enumerate(federated_train_loader):\n",
        "    model_ptr = model_virtual[data.location.id]\n",
        "    optimizer.zero_grad()     # training the model\n",
        "    output = model_ptr(data)\n",
        "\n",
        "    # this loss is a pointer to the tensor loss at the remote location\n",
        "    loss = F.nll_loss(output, target)\n",
        "    # call backward() on the loss pointer, that will send the command to call\n",
        "    # backward on the actual loss tensor present on the remote machine\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # get back the updated model/ improved model using .get() \n",
        "    model_got = model_ptr.get() \n",
        "    #Perform model weights' updates    \n",
        "    for param in model_got.parameters():\n",
        "        param.data.add_(param.grad.data)\n",
        "    #sent back the model to the RPIs\n",
        "    model_sent = model_got.send(data.location)\n",
        "    model_virtual[data.location.id] = model_sent\n",
        "\n",
        "    if batch_index % 100 == 0: \n",
        "        # the the variable loss was also created at the remote worker, \n",
        "        # so we need to explicitly get it back\n",
        "        loss = loss.get() # get back the loss\n",
        "        print('Training Epoch: {:2d} [{:5d}/{:5d} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch+1, batch_index * 64,\n",
        "            len(federated_train_loader) * 64,\n",
        "            100. * batch_index / len(federated_train_loader), loss.item()))    \n",
        "\n"
      ],
      "metadata": {
        "id": "vADLx3PdBmkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now for testing\n",
        "# we will receive the model weights that will be aggregated to form a combined model.\n",
        "\n",
        "#model.eval()\n",
        "\n",
        "test_losses_dic = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "  # test the model in each virtual worker\n",
        "  for worker in virtual_workers:\n",
        "    test_losses = 0\n",
        "    correct = 0\n",
        "    # for each RPIs train the same testset \n",
        "    for data, target in test_loader:\n",
        "      model_ptr = model_virtual[worker.id]\n",
        "      output = model_ptr(data)\n",
        "      # add losses together\n",
        "      test_losses += F.nll_loss(\n",
        "              output, target, reduction='sum').item()\n",
        "      # to get the index of the max log-probability class\n",
        "      pred = output.argmax(1, keepdim=True) \n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_losses_dic[worker.id] = test_losses / len(test_loader.dataset)\n",
        "\n",
        "for worker in virtual_workers:\n",
        "  print('\\nTest set for worker: %s Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      worker,\n",
        "      test_losses,correct,\n",
        "      len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "kaZ01vmFHpPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}